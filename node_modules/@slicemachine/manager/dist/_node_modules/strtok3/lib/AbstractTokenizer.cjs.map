{"version":3,"file":"AbstractTokenizer.cjs","sources":["../../../../../../node_modules/strtok3/lib/AbstractTokenizer.js"],"sourcesContent":["import { EndOfStreamError } from 'peek-readable';\nimport { Buffer } from 'node:buffer';\n/**\n * Core tokenizer\n */\nexport class AbstractTokenizer {\n    constructor(fileInfo) {\n        /**\n         * Tokenizer-stream position\n         */\n        this.position = 0;\n        this.numBuffer = new Uint8Array(8);\n        this.fileInfo = fileInfo ? fileInfo : {};\n    }\n    /**\n     * Read a token from the tokenizer-stream\n     * @param token - The token to read\n     * @param position - If provided, the desired position in the tokenizer-stream\n     * @returns Promise with token data\n     */\n    async readToken(token, position = this.position) {\n        const uint8Array = Buffer.alloc(token.len);\n        const len = await this.readBuffer(uint8Array, { position });\n        if (len < token.len)\n            throw new EndOfStreamError();\n        return token.get(uint8Array, 0);\n    }\n    /**\n     * Peek a token from the tokenizer-stream.\n     * @param token - Token to peek from the tokenizer-stream.\n     * @param position - Offset where to begin reading within the file. If position is null, data will be read from the current file position.\n     * @returns Promise with token data\n     */\n    async peekToken(token, position = this.position) {\n        const uint8Array = Buffer.alloc(token.len);\n        const len = await this.peekBuffer(uint8Array, { position });\n        if (len < token.len)\n            throw new EndOfStreamError();\n        return token.get(uint8Array, 0);\n    }\n    /**\n     * Read a numeric token from the stream\n     * @param token - Numeric token\n     * @returns Promise with number\n     */\n    async readNumber(token) {\n        const len = await this.readBuffer(this.numBuffer, { length: token.len });\n        if (len < token.len)\n            throw new EndOfStreamError();\n        return token.get(this.numBuffer, 0);\n    }\n    /**\n     * Read a numeric token from the stream\n     * @param token - Numeric token\n     * @returns Promise with number\n     */\n    async peekNumber(token) {\n        const len = await this.peekBuffer(this.numBuffer, { length: token.len });\n        if (len < token.len)\n            throw new EndOfStreamError();\n        return token.get(this.numBuffer, 0);\n    }\n    /**\n     * Ignore number of bytes, advances the pointer in under tokenizer-stream.\n     * @param length - Number of bytes to ignore\n     * @return resolves the number of bytes ignored, equals length if this available, otherwise the number of bytes available\n     */\n    async ignore(length) {\n        if (this.fileInfo.size !== undefined) {\n            const bytesLeft = this.fileInfo.size - this.position;\n            if (length > bytesLeft) {\n                this.position += bytesLeft;\n                return bytesLeft;\n            }\n        }\n        this.position += length;\n        return length;\n    }\n    async close() {\n        // empty\n    }\n    normalizeOptions(uint8Array, options) {\n        if (options && options.position !== undefined && options.position < this.position) {\n            throw new Error('`options.position` must be equal or greater than `tokenizer.position`');\n        }\n        if (options) {\n            return {\n                mayBeLess: options.mayBeLess === true,\n                offset: options.offset ? options.offset : 0,\n                length: options.length ? options.length : (uint8Array.length - (options.offset ? options.offset : 0)),\n                position: options.position ? options.position : this.position\n            };\n        }\n        return {\n            mayBeLess: false,\n            offset: 0,\n            length: uint8Array.length,\n            position: this.position\n        };\n    }\n}\n"],"names":["Buffer","EndOfStreamError"],"mappings":";;;;AAKO,MAAM,kBAAkB;AAAA,EAC3B,YAAY,UAAU;AAIlB,SAAK,WAAW;AAChB,SAAK,YAAY,IAAI,WAAW,CAAC;AACjC,SAAK,WAAW,WAAW,WAAW,CAAA;AAAA,EACzC;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EAOD,MAAM,UAAU,OAAO,WAAW,KAAK,UAAU;AAC7C,UAAM,aAAaA,YAAM,OAAC,MAAM,MAAM,GAAG;AACzC,UAAM,MAAM,MAAM,KAAK,WAAW,YAAY,EAAE,SAAQ,CAAE;AAC1D,QAAI,MAAM,MAAM;AACZ,YAAM,IAAIC,gBAAgB,iBAAA;AAC9B,WAAO,MAAM,IAAI,YAAY,CAAC;AAAA,EACjC;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EAOD,MAAM,UAAU,OAAO,WAAW,KAAK,UAAU;AAC7C,UAAM,aAAaD,YAAM,OAAC,MAAM,MAAM,GAAG;AACzC,UAAM,MAAM,MAAM,KAAK,WAAW,YAAY,EAAE,SAAQ,CAAE;AAC1D,QAAI,MAAM,MAAM;AACZ,YAAM,IAAIC,gBAAgB,iBAAA;AAC9B,WAAO,MAAM,IAAI,YAAY,CAAC;AAAA,EACjC;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EAMD,MAAM,WAAW,OAAO;AACpB,UAAM,MAAM,MAAM,KAAK,WAAW,KAAK,WAAW,EAAE,QAAQ,MAAM,IAAG,CAAE;AACvE,QAAI,MAAM,MAAM;AACZ,YAAM,IAAIA,gBAAgB,iBAAA;AAC9B,WAAO,MAAM,IAAI,KAAK,WAAW,CAAC;AAAA,EACrC;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EAMD,MAAM,WAAW,OAAO;AACpB,UAAM,MAAM,MAAM,KAAK,WAAW,KAAK,WAAW,EAAE,QAAQ,MAAM,IAAG,CAAE;AACvE,QAAI,MAAM,MAAM;AACZ,YAAM,IAAIA,gBAAgB,iBAAA;AAC9B,WAAO,MAAM,IAAI,KAAK,WAAW,CAAC;AAAA,EACrC;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EAMD,MAAM,OAAO,QAAQ;AACjB,QAAI,KAAK,SAAS,SAAS,QAAW;AAClC,YAAM,YAAY,KAAK,SAAS,OAAO,KAAK;AAC5C,UAAI,SAAS,WAAW;AACpB,aAAK,YAAY;AACjB,eAAO;AAAA,MACV;AAAA,IACJ;AACD,SAAK,YAAY;AACjB,WAAO;AAAA,EACV;AAAA,EACD,MAAM,QAAQ;AAAA,EAEb;AAAA,EACD,iBAAiB,YAAY,SAAS;AAClC,QAAI,WAAW,QAAQ,aAAa,UAAa,QAAQ,WAAW,KAAK,UAAU;AAC/E,YAAM,IAAI,MAAM,uEAAuE;AAAA,IAC1F;AACD,QAAI,SAAS;AACT,aAAO;AAAA,QACH,WAAW,QAAQ,cAAc;AAAA,QACjC,QAAQ,QAAQ,SAAS,QAAQ,SAAS;AAAA,QAC1C,QAAQ,QAAQ,SAAS,QAAQ,SAAU,WAAW,UAAU,QAAQ,SAAS,QAAQ,SAAS;AAAA,QAClG,UAAU,QAAQ,WAAW,QAAQ,WAAW,KAAK;AAAA,MACrE;AAAA,IACS;AACD,WAAO;AAAA,MACH,WAAW;AAAA,MACX,QAAQ;AAAA,MACR,QAAQ,WAAW;AAAA,MACnB,UAAU,KAAK;AAAA,IAC3B;AAAA,EACK;AACL;;","x_google_ignoreList":[0]}